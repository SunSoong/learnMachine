机器学习是基于统计方面知识的应用,通过相应的方法对训练集的数据进行分析


一、k-Nearest Neighbors(K近邻算法):
给定一个已知标签类的训练数据集,输入没有标签的新数据后,在训练数据集中找到与新数据最邻近的k个数据,决定新数据属于哪一类
即:由哪些离x最近的k个点来投票决定x归为哪一类

二维平面用距离计算公式:|AB|=sqrt((x1-x2)**2+(y1-y2)**2)
n维空间 用欧式距离公式:dist(x,y)=sqrt(sum.((xi-yi)**2))

步骤:
1.计算已知类别的数据集中的点与当前点之间的距离
2.按照距离递增次序排列
3.选取与当前点距离最小的k个点
4.确定钱k个点所在类别的出现频率
5.返回前k个点出现频率最高的类别作为当前点的预测类别

可能会用到:数据归一化(0-1标准化,Z-score标准化,Sigmoid压缩法)

优点:
1.简单好用,易理解,精度高,理论成熟,可以用来分类,也可以用来回归
2.可以用于数值型数据和离散数据
3.无数据输入假定
4.适合对稀有事件进行分类
缺点:
1.计算复杂性高,空间复杂性高
2.计算量太大,数值太大时不适合用,单个样本太少也易失误
3.样本不平衡问题(即有些样本数量很多,其他样本数量很少)
4.可理解性较差,无法给出数据的内在含义

二、DecisionTree(决策树)
有监督学习算法,一种基本的分类和回归方法,有分类树和回归树
决策树的建立:类似于if-else规则的集合,二叉树的建立

香农熵
熵定义为信息的期望,在信息论与概率统计中,熵是表示随机变量不确定性的度量
        l(xi)=-np.log(2,p(xi))
计算所有类别所有可能值包含的信息期望值(数学期望)
        Ent(D)=-sum(p(xi)np.log(2,p(xi)))

信息增益(Information Gain)的计算公式就是父节点的信息熵与其下所有子节点总信息熵之差
注意,子节点求和需要在汇总之前进行修正

选择最大信息增益,对数据集进行划分

递归构建决策树:
得到原始数据集,然后基于最好的属性值划分数据集,由于特征值可能多于两个,因为可能存在大于两个分支的数据集划分
第一次划分后,数据集向下传递到树的分支的下一个节点
递归出口:遍历完所有的特征列,或者每个疯子下的所有实例都是具有相同的分类

三、NaiveBayes朴素贝叶斯
贝叶斯分类算法是统计学的一种概率分类方法,朴素杯也是分类是杯也是分类中最简单的一种
原理:利用杯也是公式根据某特征的先验概率计算出其后验概率,然后萱蕚具有最大后延概率的类作为该特征的所属类
朴素--->最简单的假设:所有的特征之间是统计独立的

条件概率公式:P(A|B)=P(B|A)*P(A)/P(B)
全概率公式:P(B)=sum(P(Ai)*P(B|Ai))
==>P(Ai|B)=P(Ai)*P(B|Ai)/sum(P(Ai)P(B|Ai))
其中
P(A)为 先验概率(Prior probability) 即B事件发生之前,对A事件概率的一个判断
P(A|B)为 后验概率(Posterior probability) 即B事件发生之后,对A事件概率的重新评估
P(B|A)/P(B)为 可能性函数(Likely hood) 这是一个调整因子 使得预估概率更接近真实概率

在scikit-learn中,一共有3个贝叶斯的分类算法,分别为GaussianNB,MultinomialNB和BernoulliNB
1.Gaussian是先验为高斯分布(正态分布)的朴素贝叶斯,假设每个标签都服从简单的正态分布
2.MultinomialNB是先验为多项式分布的朴素贝叶斯,假设特征是由一个简单多项式分布生成的
    多项式分布可以描述各种类型样本出现次数的概率,因为多项式朴素贝叶斯适合用于描述出现次数或者出现次数比例的特征
    此模型常用于文本分类,特征表示的是次数,例如某个词语出现的次数
3.BernoulliNB是先验为伯努利分布的朴素贝叶斯
    在伯努利模型中,每个特征的取值为布尔型,True or False
    在文本分类中,就是一个特征有没有在一个文档中出现

summary:
1.如果样本特征的分布大部分是连续值,使用GaussianNB会更好
2.如果样本特征的分布大部分是多元离散值,使用MultinomialNB更合适
3.如果样本特征是二元离散值或者很稀疏的多元离散值,用BernoulliNB